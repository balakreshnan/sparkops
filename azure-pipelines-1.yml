# Starter pipeline
# Start with a minimal pipeline that you can customize to build and deploy your code.
# Add steps that build, run tests, deploy, and more:
# https://aka.ms/yaml

trigger:
- main

pool:
  vmImage: ubuntu-latest

variables:
  databricks-host: 'https://centralus.azuredatabricks.net'
  notebook-folder: 'notebooks/Users/babal@microsoft.com/ML/'
  cluster-id: '0203-173454-scums615'
  notebook-name: 'xgboost-python.py'

steps:
- task: UsePythonVersion@0
  displayName: 'Use Python 3.x'

- script: |
    pip install databricks-cli
  displayName: 'Install databricks-cli'

- script: |
   databricks fs cp mnt/demo/ dbfs:/tmp/demo --recursive --overwrite

   databricks workspace import_dir src/ $(notebook-folder) -o

   JOB_ID=$(databricks jobs create --json '{
     "name": "Testrun",
     "existing_cluster_id": "$(cluster-id)",
     "timeout_seconds": 3600,
     "max_retries": 1,
     "notebook_task": {
       "notebook_path": "$(notebook-folder)$(notebook-name)",
       "base_parameters": {}
     }
   }' | jq '.job_id')

   RUN_ID=$(databricks jobs run-now --job-id $JOB_ID | jq '.run_id')

   job_status="PENDING"
   while [ $job_status = "RUNNING" ] || [ $job_status = "PENDING" ]
   do
     sleep 2
     job_status=$(databricks runs get --run-id $RUN_ID | jq -r '.state.life_cycle_state')
     echo Status $job_status
   done

   RESULT=$(databricks runs get-output --run-id $RUN_ID)

   RESULT_STATE=$(echo $RESULT | jq -r '.metadata.state.result_state')
   RESULT_MESSAGE=$(echo $RESULT | jq -r '.metadata.state.state_message')
   if [ $RESULT_STATE = "FAILED" ]
   then
     echo "##vso[task.logissue type=error;]$RESULT_MESSAGE"
     echo "##vso[task.complete result=Failed;done=true;]$RESULT_MESSAGE"
   fi

   echo $RESULT | jq .
  displayName: 'Run Databricks Notebook'
  env:
    DATABRICKS_TOKEN: $(databricks-token)
    DATABRICKS_HOST: $(databricks-host)
